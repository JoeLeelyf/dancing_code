{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH.UTILS.DATA\n",
    "At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoade`r class. It represents a Python iterable over a dataset, with support for:\n",
    "- map-style and iterable-style datasets,\n",
    "- customizing data loading order,\n",
    "- automatic batching,\n",
    "- single and multi-process data loading,\n",
    "- automatic memory pinning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, \n",
    "    batch_sampler=None, num_workers=0, collate_fn=None, \n",
    "    pin_memory=False, drop_last=False, timeout=0, \n",
    "    worker_init_fn=None, *, prefetch_factor=2, \n",
    "    persistent_workers=False)\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- dataset (Dataset) – dataset from which to load the data.\n",
    "- batch_size (int, optional) – how many samples per batch to load (default: 1).\n",
    "- shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n",
    "- sampler (Sampler or Iterable, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified.\n",
    "- batch_sampler (Sampler or Iterable, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.\n",
    "- num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "- collate_fn (Callable, optional) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n",
    "- pin_memory (bool, optional) – If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below.\n",
    "- drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n",
    "- timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)\n",
    "- worker_init_fn (Callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)\n",
    "- generator (torch.Generator, optional) – If not None, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate base_seed for workers. (default: None)\n",
    "- prefetch_factor (int, optional, keyword-only arg) – Number of batches loaded in advance by each worker. 2 means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is None. Otherwise if value of num_workers>0 default is 2).\n",
    "- persistent_workers (bool, optional) – If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: False)\n",
    "- pin_memory_device (str, optional) – the data loader will copy Tensors into device pinned memory before returning them if pin_memory is set to true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types\n",
    "### Map-style datasets\n",
    "Map-style datasets with a __getitem__ method, and that support __len__, __iter__, and __getitem__ protocols. They are expected to have a `len` property, which indicates the size of the dataset. They are also expected to have a `getitem` method, which supports integer indexing in range from 0 to len(self) exclusive. The `getitem` method can take any integer value within the range and return a corresponding data sample. The `getitem` method can also take a slice object with arbitrary start, stop, and step integers. The `getitem` method can also take a list or tuple of indices and return a corresponding list of data samples. The `getitem` method can also take a boolean mask of the same length as the dataset and return a corresponding list of data samples. The `getitem` method can also take a single tuple, denoting a multi-dimensional index, and return a corresponding data sample.\n",
    "\n",
    "### Iterable-style datasets\n",
    "Iterable-style datasets that implement the __iter__ protocol, and represent an iterable over data samples. They do not need to specify a length, as they can continue to return data samples forever. They are expected to yield data samples, instead of having a `getitem` method. The yielded data samples are expected to be a tuple of (batch_data, batch_targets) by default, or a list of batch_data elements if the collate_fn option is specified."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
