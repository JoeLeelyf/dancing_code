{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH.NN\n",
    "torch.nn is a subpackage in the PyTorch library that provides various classes and functions for building and training neural networks. It stands for \"neural networks\" and is a fundamental component of PyTorch for developing deep learning models. <br>\n",
    "The torch.nn package provides a high-level API for defining and organizing neural networks, including layers, activation functions, loss functions, optimizers, and more. It allows you to design and train neural networks using an object-oriented approach. <br>\n",
    "Here are some key components and functionalities provided by the torch.nn package:\n",
    "- Neural Network Layers: torch.nn provides a wide range of pre-defined layers such as linear (fully connected), convolutional, recurrent, pooling, normalization, and activation layers. These layers can be easily combined to create complex network architectures.\n",
    "- Activation Functions: torch.nn includes popular activation functions like ReLU, sigmoid, tanh, and more. These functions introduce non-linearity to the network, enabling it to learn complex patterns and make predictions.\n",
    "- Loss Functions: torch.nn provides a variety of loss functions such as mean squared error (MSE), cross-entropy, binary cross-entropy, and more. These functions quantify the difference between the predicted output of the network and the true target values, serving as a measure of the network's performance.\n",
    "- Optimizers: torch.nn supports various optimization algorithms such as stochastic gradient descent (SGD), Adam, RMSprop, and more. These optimizers update the network's parameters based on the computed gradients during the training process, aiming to minimize the loss function.\n",
    "- Custom Networks: torch.nn allows you to define custom neural network architectures by subclassing the torch.nn.Module class. This class provides the necessary functionality to define the forward pass of the network and automatically tracks the network's parameters.\n",
    "- Data Handling: torch.nn provides classes and utilities for handling input data transformations, batching, shuffling, and more, making it easier to preprocess and feed data to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `parameter`(data:Tensor, requires_grad:bool = None)\n",
    "Parameters are Tensor subclasses, that have a very special property when used with `Module` s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in `parameters()` iterator. \n",
    "- data (Tensor) – parameter tensor.\n",
    "- requires_grad (bool, optional) – if the parameter requires gradient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # Define the parameters of the model\n",
    "        self.weight = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Create an instance of the LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# Use model.parameters() to get the parameters of the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Module` \n",
    "Base class for all neural network modules. Your models should also subclass this class. <br>\n",
    "It provides a convenient way to define neural network models, serving as a container for other modules, such as layers, activation functions, loss functions, and more. <br>\n",
    "When defining a neural network model using PyTorch, it is common practice to subclass torch.nn.Module and implement the necessary methods for the model's functionality. This approach allows for a modular and organized design, as well as easy access to common functionalities provided by `torch.nn.Module`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some key points about `torch.nn.Module`:\n",
    "- Forward Pass: Every subclass of `torch.nn.Module` must override the `forward()` method. This method is called when the model is \"called\" or invoked using the `__call__()` method. It defines the computation performed at every call, and returns the output of the model.\n",
    "- Parameters Tracking: `torch.nn.Module` tracks all the trainable parameters of the model. These parameters are defined inside the `__init__()` method of the subclass, and can be accessed using the `parameters()` or `named_parameters()` methods.\n",
    "- GPU Support: `torch.nn.Module` is compatible with GPU acceleration. You can move the entire model (including all its parameters and buffers) to a GPU device using the `to()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Sequential`\n",
    "A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an `OrderedDict` of modules can be passed in. The `forward()` method of Sequential accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential to create a small model. When `model` is run,\n",
    "# input will first be passed to `Conv2d(1,20,5)`. The output of\n",
    "# `Conv2d(1,20,5)` will be used as the input to the first\n",
    "# `ReLU`; the output of the first `ReLU` will become the input\n",
    "# for `Conv2d(20,64,5)`. Finally, the output of\n",
    "# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Conv1d`\n",
    "Applies a 1D convolution over an input signal composed of several input planes. It's designed for 1-D input data, such as sequential data (e.g. time series) or text data that can be represented as a sequence of words. It applies a convolution operation along the time dimension of the input. The convolution operation involves sliding a set of learnable filters over the input, performing element-wise multiplications and summations to produce an output feature map.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- in_channels (int) – Number of channels in the input signal. For example, for a grayscale image, the number of channels is 1, while for an RGB image, it is 3.\n",
    "- out_channels (int) – Number of channels produced by the convolution. This is equivalent to the number of filters used in the convolution operation.\n",
    "- kernel_size(int or tuple) – Size of the convolving kernel. Can be a single number or a tuple (kH, kW) where kH is the height of the kernel and kW is the width of the kernel.\n",
    "- stride(int or tuple, optional) – Stride of the convolution. Can be a single number or a tuple (sH, sW) where sH is the stride for the height dimension and sW is the stride for the width dimension. Default: 1\n",
    "- padding(int, tuple or str, optional) – Zero-padding added to both sides of the input. Can be a single number or a tuple (padH, padW), where padH is the padding in the height dimension and padW is the padding in the width dimension. Default: 0\n",
    "- padding_mode(string, optional) – Accepted values `zeros` and `circular` Default: `zeros`\n",
    "- dilation(int or tuple, optional) – Spacing between kernel elements. Can be a single number or a tuple (dH, dW), where dH is the dilation in the height dimension and dW is the dilation in the width dimension. Default: 1\n",
    "- bias(bool, optional) – If True, adds a learnable bias to the output. Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Tensor : shape **(batch_size, channels, sequence_length)**, where channels represents the number of input channels or features, and sequence_length represents the length of the sequence or time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple Conv1d model\n",
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Create an instance of the Conv1dModel\n",
    "model = Conv1dModel()\n",
    "\n",
    "# Generate dummy input data\n",
    "x = torch.randn(1, 1, 10)  # Batch size = 1, input channels = 1, sequence length = 10\n",
    "\n",
    "# Pass the input through the Conv1d layer\n",
    "output = model(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Conv2d`\n",
    "Applies a 2D convolution over an input signal composed of several input planes. Conv2d is designed for 2-dimensional input data, such as images. It applies a convolution operation over a 2-dimensional grid, typically with height and width dimensions. The convolution operation involves sliding a set of learnable filters (also known as kernels) over the input and computing element-wise multiplications and summations to produce an output feature map.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "Same with `nn.Conv1d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Tensor: shape **(batch_size, channels, height, width)**, where channels represents the number of input channels or features, and height and width represent the spatial dimensions of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple Conv2d model\n",
    "class Conv2dModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv2dModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Create an instance of the Conv2dModel\n",
    "model = Conv2dModel()\n",
    "\n",
    "# Generate dummy input data\n",
    "x = torch.randn(1, 3, 32, 32)  # Batch size = 1, input channels = 3, height = 32, width = 32\n",
    "\n",
    "# Pass the input through the Conv2d layer\n",
    "output = model(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Conv3d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.ConvTranspose1d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "Pooling layers are also important compomnent of CNN networks, following Conv layers. It's also known as subsampling or downsampling layers. They reduce the spatial dimensions (height and width) of the input, which reduces the number of parameters and computation required in the network. Pooling layers also help the network to learn spatial invariance, which is the ability to recognize objects regardless of their position in the input. Pooling layers are typically inserted between successive convolutional layers in a CNN architecture.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It usually includes two kinds of clustering methods: Max Pooling and Average Pooling. \n",
    "- Max Pooling is the most common one: takes the maximum value of the input in the pooling window as the output. \n",
    "- Average Pooling: takes the average value of the input in the pooling window as the output.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.MaxPool1d`\n",
    "Applies a 1D max pooling over an input signal composed of several input planes. MaxPool1d is designed for 1-dimensional input data, such as time series, and performs 1-dimensional pooling operation over a 1-dimensional input. The pooling operation involves sliding a 1-dimensional window over the input and taking the maximum, or largest value, within that window. The window then slides over to the next position and the process is repeated until the entire input has been covered.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- kernel_size(int or tuple) – Size of the window to take a max over. Can be a single number or a tuple (kH, kW) where kH is the size of the window height and kW is the size of the window width.\n",
    "- stride(int or tuple, optional) – Stride of the window. Can be a single number or a tuple (sH, sW) where sH is the stride of the height dimension and sW is the stride of the width dimension. Default: kernel_size\n",
    "- padding(int or tuple, optional) – Zero-padding added to both sides of the input. Can be a single number or a tuple (padH, padW), where padH is the padding in the height dimension and padW is the padding in the width dimension. Default: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.MaxPool2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.AvgPool1d`\n",
    "Applies a 1D average pooling over an input signal composed of several input planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.AvgPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个简单的卷积神经网络模型\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# 创建CNNModel实例\n",
    "model = CNNModel()\n",
    "\n",
    "# 生成虚拟输入数据\n",
    "x = torch.randn(1, 3, 32, 32)  # 批次大小为1，输入通道数为3，图像尺寸为32x32\n",
    "\n",
    "# 通过模型进行前向传播\n",
    "output = model(x)\n",
    "\n",
    "# 打印输出形状\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Activation Functions\n",
    "Non-linear activation functions are an essential component of neural networks. They introduce non-linearity into the network, allowing it to learn complex and non-linear relationships between inputs and outputs. Without non-linear activations, neural networks would behave like a linear model, which severely limits their expressive power.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.ReLU()`\n",
    "ReLU (Rectified Linear Unit): one of the most widely used activation functions. It returns the input for positive values and zero for negative values. Mathematically, ReLU(x) = max(0, x). ReLU is computationally efficient and helps with the vanishing gradient problem.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/docs/stable/_images/ReLU.png\" style=\"zoom:52%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Sigmoid()`\n",
    "The sigmoid function maps the input to a value between 0 and 1. It has a characteristic S-shaped curve. The sigmoid function is given by the formula sigmoid(x) = 1 / (1 + exp(-x)). It is commonly used in binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/docs/stable/_images/Sigmoid.png\" style=\"zoom:52%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Tanh()`\n",
    "The tanh function is similar to the sigmoid function but maps the input to a value between -1 and 1. It is given by the formula tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). Tanh is helpful in capturing negative values and is commonly used in recurrent neural networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/docs/stable/_images/Tanh.png\" style=\"zoom:52%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.LeakyReLU()`\n",
    "Leaky ReLU is a variation of the ReLU function that allows a small non-zero gradient when the input is negative. It helps address the \"dying ReLU\" problem, where neurons can become inactive during training. Mathematically, Leaky ReLU(x) = max(0.01x, x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/docs/stable/_images/LeakyReLU.png\" style=\"zoom:52%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network with tanh activation\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        # Feel free to replace `tanh` with `relu` or any other activation\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = Net()\n",
    "\n",
    "# Generate some dummy input data\n",
    "input_data = torch.randn(1, 10)\n",
    "\n",
    "# Pass the input through the network\n",
    "output = net(input_data)\n",
    "\n",
    "# Apply tanh activation to the output\n",
    "activated_output = torch.tanh(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Softmax()`\n",
    "The softmax function is commonly used in multi-class classification problems. It transforms a vector of real numbers into a probability distribution, where the sum of all the values in the output vector is equal to 1. Softmax is given by the formula softmax(x_i) = exp(x_i) / sum(exp(x_j))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0963, 0.5828, 0.3209],\n",
      "        [0.1635, 0.0498, 0.7867]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers\n",
    "Normalization layers are used to normalize the outputs of the previous layer. Normalization layers help networks train faster and reduce the chance of getting stuck in the vanishing gradient problem. They are typically inserted after convolutional layers in a CNN architecture.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- num_features – Number of channels in the input (required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network with batch normalization\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.bn1 = nn.BatchNorm1d(5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = Net()\n",
    "\n",
    "# Generate some dummy input data\n",
    "input_data = torch.randn(5, 10)\n",
    "\n",
    "# Pass the input through the network\n",
    "output = net(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers\n",
    "Recurrent layers, also known as recurrent neural network (RNN) layers, are a type of layer commonly used in neural networks for processing sequential data. Unlike feedforward neural networks that process inputs independently, recurrent layers have connections that allow information to persist across different time steps.<br>\n",
    "The key feature of recurrent layers is their ability to capture temporal dependencies and patterns in sequential data. They achieve this by maintaining an internal hidden state that is updated at each time step and serves as a form of memory. This hidden state is shared across all time steps, allowing the network to learn and remember information from previous time steps.<br>\n",
    "Recurrent layers can be used in various applications where the input data has a temporal or sequential nature, such as natural language processing, speech recognition, time series analysis, and handwriting recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.RNN()`\n",
    "Applies a multi-layer Elman RNN with tanh or ReLU to an input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- input_size – The number of expected features in the input x\n",
    "- hidden_size – The number of features in the hidden state h\n",
    "- num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "- nonelinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’\n",
    "- bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "- batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "- dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last time step\n",
    "        return output\n",
    "\n",
    "# Create an instance of the RNN model\n",
    "rnn_model = RNNModel(input_size=10, hidden_size=20, output_size=5)\n",
    "\n",
    "# Generate some dummy input data\n",
    "input_data = torch.randn(3, 6, 10)  # batch size = 3, sequence length = 6, input size = 10\n",
    "\n",
    "# Pass the input data through the model\n",
    "output = rnn_model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.LSTM()`\n",
    "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "Almost same as `nn.RNN()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden_state, cell_state) = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last time step\n",
    "        return output\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "lstm_model = LSTMModel(input_size=10, hidden_size=20, output_size=5)\n",
    "\n",
    "# Generate some dummy input data\n",
    "input_data = torch.randn(3, 6, 10)  # batch size = 3, sequence length = 6, input size = 10\n",
    "\n",
    "# Pass the input data through the model\n",
    "output = lstm_model(input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.GRU()`\n",
    "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "Almost same as `nn.RNN()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last time step\n",
    "        return output\n",
    "\n",
    "# Create an instance of the GRU model\n",
    "gru_model = GRUModel(input_size=10, hidden_size=20, output_size=5)\n",
    "\n",
    "# Generate some dummy input data\n",
    "input_data = torch.randn(3, 6, 10)  # batch size = 3, sequence length = 6, input size = 10\n",
    "\n",
    "# Pass the input data through the model\n",
    "output = gru_model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers\n",
    "A Linear Layer, also known as a Fully Connected Layer or Dense Layer, is a type of layer commonly used in neural networks for performing linear transformations on input data. It is one of the basic building blocks of a neural network.\n",
    "\n",
    "In a Linear Layer, each input neuron is connected to every neuron in the layer, and each connection is associated with a weight. The output of the layer is obtained by performing a weighted sum of the inputs, where each input is multiplied by its corresponding weight. Additionally, a bias term can be added to the sum.\n",
    "\n",
    "Mathematically, the output of a Linear Layer can be represented as:\n",
    "\n",
    "```\n",
    "output = (input * weight^T) + bias\n",
    "```\n",
    "\n",
    "where `input` is the input data, `weight` is the weight matrix, `bias` is the bias vector, and `^T` denotes the transpose operation.\n",
    "\n",
    "The purpose of a Linear Layer is to learn the optimal weights and biases that allow the neural network to map the input data to the desired output. It provides the network with the ability to learn complex relationships between the input and output data by adjusting the weights during the training process through techniques like gradient descent and backpropagation.\n",
    "\n",
    "Linear Layers are often followed by non-linear activation functions, such as ReLU (Rectified Linear Unit), to introduce non-linearity and enable the network to learn more complex patterns and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Linear()`\n",
    "Applies a linear transformation to the incoming data: y = xA^T + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- in_features(int) – size of each input sample\n",
    "- out_features(int) – size of each output sample\n",
    "- bias(bool, optional) – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Dropout()`\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.<br>\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .<br>\n",
    "Furthermore, the outputs are scaled by a factor of 1/(1-p) during training. This means that during evaluation the module simply computes an identity function.\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- p (float, optional) – Probability of an element to be zeroed. Default: 0.5\n",
    "- inplace (bool, optional) – If set to True, will do this operation in-place. Default: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4620,  2.2258,  0.1863,  0.0000,  0.0734],\n",
      "        [ 0.0000, -0.0000,  1.1545,  0.6688, -2.2079],\n",
      "        [-0.4395,  0.9746, -0.0500, -1.8308,  0.0000],\n",
      "        [ 1.5775,  0.8638, -0.0000,  0.0000, -0.0000],\n",
      "        [-2.9580, -0.3091,  1.4445, -0.0000,  0.2226]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(5, 5)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "A loss function, also known as a cost function or objective function, is a measure of how well a machine learning model is performing. It quantifies the difference between the predicted output of the model and the true output or target value.<br>\n",
    "The purpose of a loss function is to provide a numerical value that represents the model's performance, which can then be minimized or optimized during the training process. By minimizing the loss function, the model aims to improve its predictions and learn the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.MSELoss`\n",
    "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y.<br>\n",
    "The unreduced (i.e. with reduction set to 'none') loss can be described as:\n",
    "    $$loss(x, y) = L = {l_1,...,l_N}^T, l_n = (x_n - y_n)^2,$$\n",
    "where N is the batch size, If reduction is not 'none' (default 'mean'), then:\n",
    "    $$loss(x, y) = \n",
    "        \\begin{cases}\n",
    "            \\operatorname{mean}(L), &  \\text{if reduction} = \\text{'mean';}\\\\\n",
    "            \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{'sum'.}\n",
    "        \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n",
    "- reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n",
    "- reduction (string, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(1., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.ones(3, 5, requires_grad=True)\n",
    "target = torch.ones(3, 5)\n",
    "target_1 = torch.zeros(3, 5)\n",
    "output = loss(input, target)\n",
    "output_1 = loss(input, target_1)\n",
    "print(output)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
      "        [0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
      "        [0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)\n",
    "output_1.backward()\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.CrossEntropyLoss`\n",
    "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.<br>\n",
    "It is useful when training a classification problem with C classes.<br>\n",
    "If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- weight (Tensor, optional) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C. Otherwise, it is treated as if having all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6094, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.ones(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[ 0.0667,  0.0667,  0.0667,  0.0667, -0.2667],\n",
      "        [ 0.0667,  0.0667,  0.0667,  0.0667, -0.2667],\n",
      "        [-0.2667,  0.0667,  0.0667,  0.0667,  0.0667]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)\n",
    "output.backward()\n",
    "print(input.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
